{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST using pure Numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages\n",
    "\n",
    "we use MNIST dataset included with Keras, and we also use the one-hot coding method of Keras to transform Y into. a one-hot coded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadind the data, Flattening and reshaping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping data, Apply one-hot coding\n",
    "\n",
    "Y_tr_resh = Y_train_orig.reshape(60000, 1)\n",
    "Y_te_resh = Y_test_orig.reshape(10000, 1)\n",
    "Y_tr_T = to_categorical(Y_tr_resh, num_classes=10)\n",
    "Y_te_T = to_categorical(Y_te_resh, num_classes=10)\n",
    "Y_train = Y_tr_T.T\n",
    "Y_test = Y_te_T.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unrolling data and make it vector shaped, Then flattening the data\n",
    "\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "X_train = X_train_flatten / 255.\n",
    "X_test = X_test_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing helper functions: relu(z) and softmax(z) and drelu(z)\n",
    "    - We use softmax activation for output layer and relu for hidden layer, we also use drelu function for calculating thr gradient of the relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation function\n",
    "\n",
    "def softmax(u):\n",
    "    s = np.exp(u) / np.sum(np.exp(u), axis=0, keepdims=True)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "\n",
    "def relu(p):\n",
    "    r = np.maximum(0, p)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of the ReLU function\n",
    "def drelu(z):\n",
    "    z[z <= 0] = 0\n",
    "    z[z > 0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Weights and Biases for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters is a dicttionary containing weights and biases and is used throughout the code\n",
    "\n",
    "parameters = {}\n",
    "\n",
    "# layer dims in a dictionary containing size of each layer (number of hidden units and input and ouput units)\n",
    "def initialize_parameters(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        \n",
    "        # initializing W1 and W2 with random numbers and b1 and b2 with zeros.\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * (np.sqrt(2 / layer_dims[l - 1]))\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the deep network Block by Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict method which runs the final weights and biases, It is used for measuring performance over the test set\n",
    "\n",
    "def predict(parameters, X_test):\n",
    "    \n",
    "    forward_prop(parameters, X_test, activation)\n",
    "    predictions = np.round(activation[\"A4\"])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "m = X_train.shape[1]\n",
    "activation = {}\n",
    "\n",
    "# The forward propagation method (forward pass)\n",
    "def forward_prop(parameters, X_train, activation):\\\n",
    "    \n",
    "    \n",
    "    \n",
    "    outputs[\"Z\" + str(1)] = np.dot(parameters[\"W1\"], X_train) + parameters[\"b1\"]\n",
    "    activation[\"A\" + str(1)] = relu(outputs[\"Z\" + str(1)])\n",
    "    \n",
    "    for l in range(2, 4):\n",
    "        outputs[\"Z\" + str(l)] = np.dot(parameters[\"W\" + str(l)], activation[\"A\" + str(l - 1)]) + parameters[\"b\" + str(l)]\n",
    "        activation[\"A\" + str(l)] = relu(outputs[\"Z\" + str(l)])\n",
    "        \n",
    "    outputs[\"Z4\"] = np.dot(parameters[\"W4\"], activation[\"A3\"]) + parameters[\"b4\"]\n",
    "    activation[\"A4\"] = softmax(outputs[\"Z4\"])\n",
    "    \n",
    "    return outputs, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost method which computes cost using Logistic Loss function (Cross entropy loss)\n",
    "\n",
    "def compute_cost(activation):\n",
    "    loss = - np.sum((Y_train * np.log(activation[\"A4\"])), axis=0, keepdims=True)\n",
    "    cost = np.sum(loss, axis=1) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing all the needed gradient(derivatives), This method is actually the backward pass (Back Propagation)\n",
    "\n",
    "grad_reg = {}\n",
    "m = X_train.shape[1]\n",
    "\n",
    "def back_prop(parameters, outputs, activation):\n",
    "    \n",
    "    grad_reg[\"dZ4\"] = (activation[\"A4\"] - Y_train) / m\n",
    "    for l in range(1, 4):\n",
    "        grad_reg[\"dA\" + str(4 - l)] = np.dot(parameters[\"W\" + str(4 - l + 1)].T, grad_reg[\"dZ\" + str(4 - l + 1)])\n",
    "        grad_reg[\"dZ\" + str(4 - l)] = grad_reg[\"dA\" + str(4 - l)] * drelu(outputs[\"Z\" + str(4 - l)])\n",
    "        \n",
    "    grad_reg[\"dW1\"] = np.dot(grad_reg[\"dZ1\"], X_train.T)\n",
    "    grad_reg[\"db1\"] = np.sum(grad_reg[\"dZ1\"], axis=1, keepdims=True)\n",
    "    \n",
    "    for l in range(2, 5):\n",
    "        grad_reg[\"dW\" + str(l)] = np.dot(grad_reg[\"dZ\" + str(l)], activation[\"A\" + str(l - 1)].T)\n",
    "        grad_reg[\"db\" + str(l)] = np.sum(grad_reg[\"dZ\" + str(l)], axis=1, keepdims=True)\n",
    "        \n",
    "    return parameters, outputs, activation, grad_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definning the optimize function which performs gradient descent on the data and fits it to the model\n",
    "\n",
    "def optimize(grad_reg, learning_rate=0.005):\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - (learning_rate * grad_reg[\"dW\" + str(i)])\n",
    "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - (learning_rate * grad_reg[\"db\" + str(i)])\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all the blocks togather, Forming the neural network model\n",
    "    - The model it self which uses all the above functions and performs these actions in row:\n",
    "        1. Calculate output using Forward Propagation\n",
    "        2. Calculate cost using Compute Cost\n",
    "        3. Calculate gradients using Back Propagation\n",
    "        4. Updating Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "num_iterations = 1000\n",
    "costs = []\n",
    "\n",
    "# The whole model\n",
    "\n",
    "def model(num_iterations, costs, activation):\n",
    "    \n",
    "    initialize_parameters([X_train.shape[0], 50, 50, 50, 10])\n",
    "    \n",
    "    for l in range(0, num_iterations):\n",
    "        \n",
    "        forward_prop(parameters, X_train, activation)\n",
    "        cost = compute_cost(activation)\n",
    "        back_prop(parameters, outputs, activation)\n",
    "        optimize(grad_reg, learning_rate=0.005)\n",
    "        \n",
    "        if l % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "        if l % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (l, cost))\n",
    "            \n",
    "    return costs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running and feeding the data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.406661\n",
      "Cost after iteration 100: 2.121362\n"
     ]
    }
   ],
   "source": [
    "# Calling the model the feeding it data to train\n",
    "c, p = model(num_iterations, costs, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "    - Now that the model is trained completely, We use the Test set to measure how well the model is trained, and we plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 90.00116666666666 %\n",
      "test accuracy: 90.006 %\n"
     ]
    }
   ],
   "source": [
    "# Calling predict function on the test set using trained parameters of the network\n",
    "\n",
    "Y_prediction_train = predict(p, X_train)\n",
    "Y_prediction_test = predict(p, X_test)\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-7b35fbe5c1a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plotting the learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iterations (per hundreds)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting the learning curve\n",
    "plt.plot(c)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.axis([0, num_iterations/100 + 0.1, 0, 3])\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
